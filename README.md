# DCASE Challenge 2019 - Task 2: Audio Tagging
We're students at the Institute of Computational Perception of the JKU Linz, participating in the 2019 DCASE Challenge, Task 2. This repository contains the code for our submission.

## Getting Started
### Prerequisites
This project is written in Python, with the help of `keras` and a `tensorflow` backend. 
In addition to that, we use `sox` and `librosa` to perform pre-processing of the audio
data. If you want to run this project, you also need the (Python) libraries `numpy`, 
`h5py` and `tqdm`. 

### Installation
The easiest way to set everything up for this project is to create a `conda` environment,
containing all necessary libraries. For this, you can run `install_dependencies.sh`, e.g. with

```
sh install_dependencies.sh
```

### Data
After setting up the environment, download the data for 2019 DCASE Task 2 [here](https://www.kaggle.com/c/freesound-audio-tagging-2019/data).

## Run Experiments

### Cross Validation Setup
In order to determine the performance of our system, we used 4-fold cross validation. The four 
folds are stored in `datasets/cv`, and are generated by running `audiotagging/data/preprocess_data.py`, e.g. with

```
python preprocess_data.py
```

After running this, you should have various files in the cross validation directory. As noisy
files can only be used for training (for all four folds), files with a name as `foldx_curated_train`
are used to obtain audio file names for training, and `foldx_curated_eval` contain the 
appropriate validation files for the current fold `x`.

### Extract Features
For our final audio-tagging system, we used 5 different features extracted from raw audio files. To
find the exact parameters, see our technical report (link follows). In order to extract any feature,
you will need to run `audiotagging/data/feature_extractor.py`; for Mel spectrograms, this could look like

```
python feature_extractor.py --melspectrogram
python feature_extractor.py --melspectrogram --noisy
```

to obtain features for both the curated, as well as the noisy data. For constant Q-power spectra, 
`--melspectrogram` needs to be changed to `--cqt`. To compute perceptually weighted features,
use the option `--spec_weighting`. 

### Load (Pre-)Trained Models
In case you want to directly perform tests with our models, look into the folder `final_models`.
You can simply load the models e.g by
```
from keras.models import load_model

network = load_model('final_models/mel/shallow_cnn_swa_fold1.hd5')
```

### Train Models
To train a model, you can run
```
python train.py shallow_cnn.py
```
where `shallow_cnn.py` is our average-pooled CNN, and can also be replaced by `cpjku_2018_dense.py`
to train our max-pooled CNN. In case you want to experiment with and different parameters, 
you can modify `audiotagging/models/defaults.vars`.

### Predict and Evaluate
In order to compute and save predictions with a trained model, you can use `audiotagging/predict.py`, e.g. by
```
python predict.py ../final_models/mel/shallow_cnn_swa_fold1.hd5
```
where the last argument needs to be a path to a trained model file. Similar to this,
`audiotagging/evaluate.py` can be used to evaluate these predictions given ground-truth, where the last
argument needs to point to the previously stored predictions.

## Acknowledgements
Thanks to everyone at the CP Institute at the JKU, in particular Andreas, Hamid, Khaled, and also to the authors of the [second place](http://dcase.community/documents/challenge2018/technical_reports/DCASE2018_Dorfer_999.pdf) of the 2018 DCASE Challenge Task 2.
